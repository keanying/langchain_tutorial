"""
LangSmithè¯„ä¼°ç¤ºä¾‹

æœ¬ç¤ºä¾‹å±•ç¤ºå¦‚ä½•ä½¿ç”¨LangSmithè¯„ä¼°LLMåº”ç”¨ç¨‹åºçš„æ€§èƒ½ï¼Œ
åŒ…æ‹¬åˆ›å»ºè¯„ä¼°æ•°æ®é›†ã€è¿è¡Œè‡ªåŠ¨è¯„ä¼°å’Œåˆ†æè¯„ä¼°ç»“æœã€‚
"""

import os
from typing import Dict, Any, List, Optional
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain
from langchain.smith import RunEvalConfig, run_on_dataset
from langchain.evaluation import EvaluatorType
from langchain.evaluation.criteria import LabeledCriteriaEvalChain
from langsmith import Client
import pandas as pd
import matplotlib.pyplot as plt

# è®¾ç½®LangSmithç¯å¢ƒå˜é‡
os.environ["LANGCHAIN_API_KEY"] = "your-langsmith-api-key"  # è¯·æ›¿æ¢ä¸ºæ‚¨çš„LangSmith APIå¯†é’¥
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "langsmith-evaluation"


def create_qa_chain():
    """åˆ›å»ºç”¨äºé—®ç­”çš„ç®€å•é“¾"""
    # åˆ›å»ºLLMå®ä¾‹
    llm = ChatOpenAI(temperature=0.7)

    # åˆ›å»ºæç¤ºæ¨¡æ¿
    template = """ä½œä¸ºä¸€ä¸ªçŸ¥è¯†æ¸Šåšçš„AIåŠ©æ‰‹ï¼Œè¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š

é—®é¢˜: {question}

è¯·æä¾›å‡†ç¡®ã€å…¨é¢ä½†ç®€æ´çš„å›ç­”ã€‚"""
    prompt = ChatPromptTemplate.from_template(template)

    # åˆ›å»ºé“¾
    chain = LLMChain(llm=llm, prompt=prompt)

    return chain


def create_evaluation_dataset():
    """åˆ›å»ºè¯„ä¼°æ•°æ®é›†"""
    print("ğŸ” åˆ›å»ºè¯„ä¼°æ•°æ®é›†...")

    # åˆ›å»ºLangSmithå®¢æˆ·ç«¯
    client = Client()

    # å®šä¹‰é—®é¢˜å’Œå‚è€ƒç­”æ¡ˆ
    qa_pairs = [
        {
            "question": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            "reference": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿè‡ªåŠ¨ä»ç»éªŒä¸­å­¦ä¹ å’Œæ”¹è¿›ï¼Œè€Œæ— éœ€æ˜¾å¼ç¼–ç¨‹ã€‚å®ƒä¸»è¦å…³æ³¨å¼€å‘èƒ½å¤Ÿè®¿é—®æ•°æ®å¹¶ä½¿ç”¨æ•°æ®è‡ªæˆ‘å­¦ä¹ çš„ç®—æ³•ã€‚"
        },
        {
            "question": "ä»€ä¹ˆæ˜¯ç¥ç»ç½‘ç»œï¼Ÿ",
            "reference": "ç¥ç»ç½‘ç»œæ˜¯ä¸€ç§å—äººè„‘ä¸­ç¥ç»å…ƒç½‘ç»œå¯å‘çš„è®¡ç®—æ¨¡å‹ã€‚å®ƒç”±å¤šå±‚äº’è¿çš„èŠ‚ç‚¹ï¼ˆæˆ–'ç¥ç»å…ƒ'ï¼‰ç»„æˆï¼Œå¯ä»¥è¯†åˆ«è¾“å…¥æ•°æ®ä¸­çš„æ¨¡å¼ã€‚ç¥ç»ç½‘ç»œæ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€ï¼Œå¹¿æ³›åº”ç”¨äºå›¾åƒè¯†åˆ«ã€è¯­è¨€å¤„ç†ç­‰é¢†åŸŸã€‚"
        },
        {
            "question": "å¼ºåŒ–å­¦ä¹ æ˜¯ä»€ä¹ˆï¼Ÿ",
            "reference": "å¼ºåŒ–å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ç§å½¢å¼ï¼Œæ™ºèƒ½ä½“é€šè¿‡ä¸ç¯å¢ƒäº¤äº’å¹¶ä»è¡ŒåŠ¨çš„ç»“æœä¸­å­¦ä¹ ã€‚å®ƒåŸºäºå¥–åŠ±æœºåˆ¶ï¼Œæ™ºèƒ½ä½“å­¦ä¼šé€‰æ‹©èƒ½å¤Ÿæœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ±çš„è¡ŒåŠ¨ã€‚è¿™ç§æ–¹æ³•åœ¨æ¸¸æˆã€æœºå™¨äººæŠ€æœ¯å’Œè‡ªåŠ¨æ§åˆ¶ç³»ç»Ÿä¸­ç‰¹åˆ«æœ‰ç”¨ã€‚"
        },
        {
            "question": "ä»€ä¹ˆæ˜¯è‡ªç„¶è¯­è¨€å¤„ç†ï¼Ÿ",
            "reference": "è‡ªç„¶è¯­è¨€å¤„ç†(NLP)æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œä¸“æ³¨äºè®¡ç®—æœºç†è§£ã€è§£é‡Šå’Œç”Ÿæˆäººç±»è¯­è¨€çš„èƒ½åŠ›ã€‚NLPæŠ€æœ¯ä½¿åº”ç”¨ç¨‹åºèƒ½å¤Ÿè¯†åˆ«æ–‡æœ¬ä¸­çš„æƒ…æ„Ÿã€æå–å…³é”®ä¿¡æ¯ã€ç¿»è¯‘è¯­è¨€ï¼Œç”šè‡³ç”Ÿæˆç±»ä¼¼äººç±»çš„æ–‡æœ¬å“åº”ã€‚"
        },
        {
            "question": "ä»€ä¹ˆæ˜¯è®¡ç®—æœºè§†è§‰ï¼Ÿ",
            "reference": "è®¡ç®—æœºè§†è§‰æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½é¢†åŸŸï¼Œè‡´åŠ›äºä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°å­—å›¾åƒæˆ–è§†é¢‘ä¸­è·å–é«˜çº§ä¿¡æ¯ï¼Œå¹¶åšå‡ºå†³ç­–ã€‚å®ƒæ¶‰åŠå¼€å‘ç®—æ³•æ¥ç†è§£è§†è§‰å†…å®¹ï¼Œä¾‹å¦‚å¯¹è±¡è¯†åˆ«ã€åœºæ™¯ç†è§£å’Œå›¾åƒåˆ†ç±»ã€‚"
        }
    ]

    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²å­˜åœ¨
    try:
        dataset = client.read_dataset("AIçŸ¥è¯†é—®ç­”è¯„ä¼°æ•°æ®é›†")
        print(f"âœ… æ•°æ®é›†å·²å­˜åœ¨ï¼ŒID: {dataset.id}")

    except:
        # åˆ›å»ºæ–°æ•°æ®é›†
        dataset = client.create_dataset(
            "AIçŸ¥è¯†é—®ç­”è¯„ä¼°æ•°æ®é›†",
            description="ç”¨äºè¯„ä¼°AIç³»ç»Ÿå›ç­”äººå·¥æ™ºèƒ½ç›¸å…³é—®é¢˜çš„èƒ½åŠ›çš„æ•°æ®é›†"
        )
        print(f"âœ… åˆ›å»ºäº†æ–°æ•°æ®é›†ï¼ŒID: {dataset.id}")

        # æ·»åŠ ç¤ºä¾‹åˆ°æ•°æ®é›†
        for pair in qa_pairs:
            client.create_example(
                inputs={"question": pair["question"]},
                outputs={"reference": pair["reference"]},
                dataset_id=dataset.id
            )

        print(f"âœ… å·²æ·»åŠ  {len(qa_pairs)} ä¸ªç¤ºä¾‹åˆ°æ•°æ®é›†")

    return dataset.id


def run_chain_evaluation(dataset_id, chain_name="é»˜è®¤QAé“¾"):
    """åœ¨æ•°æ®é›†ä¸Šè¯„ä¼°é“¾çš„æ€§èƒ½"""
    print(f"\nğŸ” åœ¨æ•°æ®é›†ä¸Šè¯„ä¼°{chain_name}...")

    # åˆ›å»ºé—®ç­”é“¾
    chain = create_qa_chain()

    # é…ç½®è¯„ä¼°
    evaluation_config = RunEvalConfig(
        # æ·»åŠ é¢„å®šä¹‰çš„è¯„ä¼°å™¨
        evaluators=[
            # æ­£ç¡®æ€§è¯„ä¼° - æ£€æŸ¥å›ç­”æ˜¯å¦ä¸å‚è€ƒç­”æ¡ˆä¸€è‡´
            EvaluatorType.QA,

            # ä»¥ä¸‹æ˜¯åŸºäºç‰¹å®šæ ‡å‡†çš„è¯„ä¼°å™¨
            EvaluatorType.CRITERIA.with_config({
                "criteria": {
                    "å‡†ç¡®æ€§": "å›ç­”åŒ…å«å‡†ç¡®çš„äº‹å®ä¿¡æ¯ï¼Œæ²¡æœ‰é”™è¯¯",
                    "å®Œæ•´æ€§": "å›ç­”å…¨é¢åœ°è¦†ç›–äº†é—®é¢˜çš„å„ä¸ªæ–¹é¢",
                    "ç®€æ´æ€§": "å›ç­”ç®€æ´æ˜äº†ï¼Œæ²¡æœ‰ä¸å¿…è¦çš„å†—ä½™",
                    "æœ‰ç”¨æ€§": "å›ç­”å¯¹ç”¨æˆ·æœ‰å®é™…å¸®åŠ©"
                }
            }),

            # å¯ä¿¡åº¦è¯„ä¼°
            EvaluatorType.EMBEDDING_DISTANCE,
        ],

        # è‡ªå®šä¹‰è¯„ä¼°é“¾
        custom_evaluators=[
            create_custom_evaluator()
        ]
    )

    # è¿è¡Œè¯„ä¼°
    results = run_on_dataset(
        dataset_name="AIçŸ¥è¯†é—®ç­”è¯„ä¼°æ•°æ®é›†",  # ç›´æ¥ä½¿ç”¨æ•°æ®é›†åç§°
        llm_or_chain_factory=lambda: chain,
        evaluation=evaluation_config,
        verbose=True,
        project_name=f"evaluation-{chain_name}"
    )

    print(f"âœ… {chain_name}è¯„ä¼°å®Œæˆ")

    return results


def create_custom_evaluator():
    """åˆ›å»ºè‡ªå®šä¹‰è¯„ä¼°å™¨ï¼Œè¯„ä¼°å›ç­”çš„æ•™å­¦ä»·å€¼"""
    # åˆ›å»ºç”¨äºè¯„ä¼°çš„LLM
    eval_llm = ChatOpenAI(temperature=0)

    # å®šä¹‰è¯„ä¼°æ ‡å‡†å’Œæç¤º
    criteria = {
        "æ•™å­¦ä»·å€¼": "è¯„ä¼°å›ç­”æ˜¯å¦èƒ½å¤Ÿæœ‰æ•ˆåœ°æ•™è‚²è¯»è€…ï¼Œè§£é‡Šæ¦‚å¿µå¹¶å¸®åŠ©è¯»è€…ç†è§£ä¸»é¢˜"
    }

    # åˆ›å»ºåŸºäºæ ‡å‡†çš„è¯„ä¼°é“¾
    evaluator = LabeledCriteriaEvalChain.from_llm(
        llm=eval_llm,
        criteria=criteria,
        evaluation_entities={"å›ç­”": "å›ç­”", "å‚è€ƒ": "reference"},
        output_key="æ•™å­¦ä»·å€¼"
    )

    return evaluator


def compare_models():
    """æ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ€§èƒ½"""
    print("\nğŸ” æ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ€§èƒ½...")

    # åˆ›å»ºæ•°æ®é›†
    dataset_id = create_evaluation_dataset()

    # å®šä¹‰è¦æ¯”è¾ƒçš„æ¨¡å‹
    models = {
        "æ ‡å‡†æ¸©åº¦æ¨¡å‹": lambda: ChatOpenAI(temperature=0.7),
        "ä½æ¸©åº¦æ¨¡å‹": lambda: ChatOpenAI(temperature=0.2),
        "GPT-4æ¨¡å‹": lambda: ChatOpenAI(model="gpt-4", temperature=0.7)  # è¿™åªæ˜¯ç¤ºä¾‹ï¼Œè¯·æ ¹æ®éœ€è¦è°ƒæ•´
    }

    results = {}

    # ä¸ºæ¯ä¸ªæ¨¡å‹åˆ›å»ºå’Œè¯„ä¼°é“¾
    for model_name, model_factory in models.items():
        print(f"\nè¯„ä¼° {model_name}...")

        # åˆ›å»ºä½¿ç”¨ç‰¹å®šæ¨¡å‹çš„é“¾
        def create_chain_with_model():
            llm = model_factory()
            template = """ä½œä¸ºä¸€ä¸ªçŸ¥è¯†æ¸Šåšçš„AIåŠ©æ‰‹ï¼Œè¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š

é—®é¢˜: {question}

è¯·æä¾›å‡†ç¡®ã€å…¨é¢ä½†ç®€æ´çš„å›ç­”ã€‚"""
            prompt = ChatPromptTemplate.from_template(template)
            return LLMChain(llm=llm, prompt=prompt)

        # é…ç½®è¯„ä¼°
        evaluation_config = RunEvalConfig(
            evaluators=[
                EvaluatorType.QA,
                EvaluatorType.CRITERIA.with_config({
                    "criteria": {
                        "å‡†ç¡®æ€§": "å›ç­”åŒ…å«å‡†ç¡®çš„äº‹å®ä¿¡æ¯ï¼Œæ²¡æœ‰é”™è¯¯",
                        "å®Œæ•´æ€§": "å›ç­”å…¨é¢åœ°è¦†ç›–äº†é—®é¢˜çš„å„ä¸ªæ–¹é¢",
                        "ç®€æ´æ€§": "å›ç­”ç®€æ´æ˜äº†ï¼Œæ²¡æœ‰ä¸å¿…è¦çš„å†—ä½™"
                    }
                })
            ]
        )

        # è¿è¡Œè¯„ä¼°
        result = run_on_dataset(
            dataset_name="AIçŸ¥è¯†é—®ç­”è¯„ä¼°æ•°æ®é›†",
            llm_or_chain_factory=create_chain_with_model,
            evaluation=evaluation_config,
            verbose=False,
            project_name=f"model-comparison-{model_name}"
        )

        results[model_name] = result
        print(f"âœ… {model_name} è¯„ä¼°å®Œæˆ")

    # åˆ†æç»“æœ
    analyze_comparison_results(results)

    return results


def analyze_comparison_results(results):
    """åˆ†æå’Œå¯è§†åŒ–æ¯”è¾ƒç»“æœ"""
    # æå–æ¯ä¸ªæ¨¡å‹çš„è¯„ä¼°åˆ†æ•°
    model_scores = {}

    for model_name, result in results.items():
        # è·å–ç»“æœæ•°æ®å¸§
        try:
            df = result.get_aggregate_feedback()

            # æå–å…³é”®è¯„åˆ†
            model_scores[model_name] = {
                "å‡†ç¡®æ€§": df.get("criteria:å‡†ç¡®æ€§", {}).get("mean", 0),
                "å®Œæ•´æ€§": df.get("criteria:å®Œæ•´æ€§", {}).get("mean", 0),
                "ç®€æ´æ€§": df.get("criteria:ç®€æ´æ€§", {}).get("mean", 0),
                "QAåˆ†æ•°": df.get("qa", {}).get("mean", 0)
            }
        except Exception as e:
            print(f"åˆ†æ {model_name} ç»“æœæ—¶å‡ºé”™: {str(e)}")
            model_scores[model_name] = {"å‡†ç¡®æ€§": 0, "å®Œæ•´æ€§": 0, "ç®€æ´æ€§": 0, "QAåˆ†æ•°": 0}

    # æ‰“å°æ¯”è¾ƒç»“æœ
    print("\n==== æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ ====")

    for model_name, scores in model_scores.items():
        print(f"\n{model_name}:")
        for metric, score in scores.items():
            print(f"  {metric}: {score:.2f}")

    # åˆ›å»ºå¯è§†åŒ–æ¯”è¾ƒ
    try:
        metrics = ["å‡†ç¡®æ€§", "å®Œæ•´æ€§", "ç®€æ´æ€§", "QAåˆ†æ•°"]
        model_names = list(model_scores.keys())

        # åˆ›å»ºä¸€ä¸ªå›¾è¡¨
        fig, ax = plt.subplots(figsize=(12, 8))

        # æ¡å½¢å›¾å®½åº¦
        bar_width = 0.2

        # è®¾ç½®ä½ç½®
        positions = list(range(len(metrics)))

        # ä¸ºæ¯ä¸ªæ¨¡å‹ç»˜åˆ¶æ¡å½¢å›¾
        for i, model_name in enumerate(model_names):
            values = [model_scores[model_name][metric] for metric in metrics]
            ax.bar([p + i * bar_width for p in positions], values, bar_width, label=model_name)

        # æ·»åŠ æ ‡ç­¾å’Œå›¾ä¾‹
        ax.set_ylabel('åˆ†æ•°')
        ax.set_title('ä¸åŒæ¨¡å‹åœ¨å„è¯„ä¼°æŒ‡æ ‡ä¸Šçš„æ€§èƒ½')
        ax.set_xticks([p + bar_width for p in positions])
        ax.set_xticklabels(metrics)
        ax.legend()

        # ä¿å­˜å›¾è¡¨
        plt.tight_layout()
        plt.savefig('model_comparison.png')
        print("\nâœ… æ€§èƒ½æ¯”è¾ƒå›¾å·²ä¿å­˜ä¸º 'model_comparison.png'")

    except Exception as e:
        print(f"åˆ›å»ºå¯è§†åŒ–æ—¶å‡ºé”™: {str(e)}")


def analyze_evaluation_results(results):
    """åˆ†æè¯„ä¼°ç»“æœå¹¶æä¾›è§è§£"""
    print("\nğŸ” åˆ†æè¯„ä¼°ç»“æœ...")

    try:
        # è·å–èšåˆåé¦ˆ
        feedback_df = results.get_aggregate_feedback()

        print("\n==== è¯„ä¼°ç»“æœæ‘˜è¦ ====")
        for key, value in feedback_df.items():
            if isinstance(value, dict) and "mean" in value:
                print(f"{key}: {value['mean']:.2f}")

        # è·å–è¯¦ç»†åé¦ˆ
        detailed_df = results.get_feedback()

        # æ‰¾å‡ºè¡¨ç°æœ€å¥½å’Œæœ€å·®çš„é—®é¢˜
        if "qa" in detailed_df.columns:
            best_idx = detailed_df["qa"].astype(float).idxmax()
            worst_idx = detailed_df["qa"].astype(float).idxmin()

            print("\nè¡¨ç°æœ€å¥½çš„é—®é¢˜:")
            print(f"é—®é¢˜: {detailed_df.loc[best_idx, 'input.question']}")
            print(f"å›ç­”: {detailed_df.loc[best_idx, 'output.text']}")
            print(f"QAå¾—åˆ†: {detailed_df.loc[best_idx, 'qa']}")

            print("\nè¡¨ç°æœ€å·®çš„é—®é¢˜:")
            print(f"é—®é¢˜: {detailed_df.loc[worst_idx, 'input.question']}")
            print(f"å›ç­”: {detailed_df.loc[worst_idx, 'output.text']}")
            print(f"QAå¾—åˆ†: {detailed_df.loc[worst_idx, 'qa']}")

        # æ£€æŸ¥ä¸åŒæ ‡å‡†ä¹‹é—´çš„ç›¸å…³æ€§
        criteria_cols = [col for col in detailed_df.columns if col.startswith("criteria:")]
        if len(criteria_cols) >= 2:
            print("\næ ‡å‡†ä¹‹é—´çš„ç›¸å…³æ€§:")
            criteria_data = detailed_df[criteria_cols].astype(float)
            corr = criteria_data.corr()
            print(corr)

        print("\nâœ… è¯„ä¼°åˆ†æå®Œæˆ")

    except Exception as e:
        print(f"åˆ†æç»“æœæ—¶å‡ºé”™: {str(e)}")


def custom_feedback_collection():
    """å±•ç¤ºå¦‚ä½•æ”¶é›†å’Œä½¿ç”¨è‡ªå®šä¹‰åé¦ˆ"""
    print("\nğŸ” æ¼”ç¤ºè‡ªå®šä¹‰åé¦ˆæ”¶é›†...")

    # åˆ›å»ºLangSmithå®¢æˆ·ç«¯
    client = Client()

    # åˆ›å»ºé—®ç­”é“¾
    chain = create_qa_chain()

    # å‡†å¤‡ç¤ºä¾‹æŸ¥è¯¢
    query = "è¯·è§£é‡Šå·ç§¯ç¥ç»ç½‘ç»œçš„å·¥ä½œåŸç†"

    # è¿è¡Œé“¾å¹¶å¾—åˆ°å“åº”
    response = chain.invoke({"question": query})

    # æ‰“å°å“åº”
    print("\né—®é¢˜:", query)
    print("å›ç­”:", response["text"])

    # å‡è®¾è¿™æ˜¯ä¸€ä¸ªåº”ç”¨ç¨‹åºï¼Œç”¨æˆ·å¯ä»¥æä¾›åé¦ˆ
    print("\næ¨¡æ‹Ÿç”¨æˆ·æä¾›åé¦ˆ...")

    # è·å–æœ€è¿‘çš„è¿è¡Œï¼ˆè¿™åº”è¯¥æ˜¯æˆ‘ä»¬åˆšåˆšæ‰§è¡Œçš„ï¼‰
    runs = list(client.list_runs(
        project_name=os.environ["LANGCHAIN_PROJECT"],
        execution_order=1,
        limit=1
    ))

    if runs:
        run_id = runs[0].id

        # è®°å½•æ­£é¢åé¦ˆ
        client.create_feedback(
            run_id=run_id,
            key="user_rating",
            value=4,  # 1-5è¯„åˆ†
            comment="è§£é‡Šå¾ˆæ¸…æ™°ï¼Œä½†å¸Œæœ›æœ‰æ›´å¤šå…·ä½“ç¤ºä¾‹"
        )

        # è®°å½•åˆ†ç±»åé¦ˆ
        client.create_feedback(
            run_id=run_id,
            key="æŠ€æœ¯å‡†ç¡®æ€§",
            value="é«˜",  # å¯ä»¥æ˜¯ä»»ä½•å€¼ï¼š"é«˜"/"ä¸­"/"ä½"
            comment="æŠ€æœ¯ç»†èŠ‚å‡†ç¡®"
        )

        # è®°å½•äºŒå…ƒåé¦ˆ
        client.create_feedback(
            run_id=run_id,
            key="æ»¡è¶³éœ€æ±‚",
            value=True,  # å¸ƒå°”å€¼
            comment="å›ç­”æ»¡è¶³äº†æˆ‘çš„åŸºæœ¬éœ€æ±‚"
        )

        print("âœ… ç”¨æˆ·åé¦ˆå·²è®°å½•")

        # æ£€ç´¢åé¦ˆ
        feedbacks = list(client.list_feedback(run_id=run_id))

        print("\nå·²æ”¶é›†çš„åé¦ˆ:")
        for feedback in feedbacks:
            print(f"- {feedback.key}: {feedback.value} ({feedback.comment})")

    else:
        print("âŒ æœªæ‰¾åˆ°æœ€è¿‘çš„è¿è¡Œè®°å½•")


def main():
    """ä¸»å‡½æ•°ï¼Œè¿è¡Œæ‰€æœ‰è¯„ä¼°ç¤ºä¾‹"""
    print("===== LangSmith è¯„ä¼°ç¤ºä¾‹ =====\n")

    # æ£€æŸ¥APIå¯†é’¥
    if os.environ.get("LANGCHAIN_API_KEY") == "your-langsmith-api-key":
        print("âš ï¸ è­¦å‘Š: è¯·å°†ä»£ç ä¸­çš„'your-langsmith-api-key'æ›¿æ¢ä¸ºæ‚¨çš„å®é™…LangSmith APIå¯†é’¥")
        print("ç”³è¯·APIå¯†é’¥: https://smith.langchain.com/\n")
        return

    # åˆ›å»ºè¯„ä¼°æ•°æ®é›†
    dataset_id = create_evaluation_dataset()

    # è¿è¡ŒåŸºæœ¬è¯„ä¼°
    results = run_chain_evaluation(dataset_id)

    # åˆ†æç»“æœ
    analyze_evaluation_results(results)

    # æ”¶é›†è‡ªå®šä¹‰åé¦ˆ
    custom_feedback_collection()

    # è¿è¡Œæ¨¡å‹æ¯”è¾ƒ
    # æ³¨æ„ï¼šè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´
    compare_results = compare_models()

    print("\n===== æ‰€æœ‰è¯„ä¼°ç¤ºä¾‹å®Œæˆ =====")
    print("ç™»å½• https://smith.langchain.com/ æŸ¥çœ‹è¯¦ç»†è¯„ä¼°ç»“æœ")


if __name__ == "__main__":
    main()
