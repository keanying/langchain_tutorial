"""
LangGraphå’ŒLangSmithé›†æˆç¤ºä¾‹

æœ¬ç¤ºä¾‹å±•ç¤ºå¦‚ä½•å°†LangGraphå·¥ä½œæµä¸LangSmithé›†æˆï¼Œä»¥å®ç°é«˜çº§ç›‘æ§ã€
è·Ÿè¸ªå’Œè¯„ä¼°åŠŸèƒ½ã€‚ç¤ºä¾‹æ„å»ºäº†ä¸€ä¸ªç®€å•çš„å¤šæ­¥éª¤æ¨ç†å·¥ä½œæµï¼Œå¹¶ä½¿ç”¨
LangSmithè·Ÿè¸ªå…¶æ‰§è¡Œè¿‡ç¨‹å’Œæ€§èƒ½ã€‚
"""

import os
from typing import TypedDict, List, Dict, Any, Optional, Annotated
from datetime import datetime
import pandas as pd
import matplotlib.pyplot as plt

# LangChainå’ŒLangGraphå¯¼å…¥
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import SystemMessage, HumanMessage
from langgraph.graph import StateGraph, END

# LangSmithå¯¼å…¥
from langsmith import Client
from langchain.smith import RunEvalConfig, run_on_dataset

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆç”Ÿäº§ç¯å¢ƒä¸­åº”é€šè¿‡å…¶ä»–å®‰å…¨æ–¹å¼è®¾ç½®ï¼‰
os.environ["OPENAI_API_KEY"] = "YOUR_OPENAI_API_KEY"  # è¯·æ›¿æ¢ä¸ºæ‚¨çš„OpenAIå¯†é’¥
os.environ["LANGCHAIN_API_KEY"] = "YOUR_LANGSMITH_API_KEY"  # è¯·æ›¿æ¢ä¸ºæ‚¨çš„LangSmithå¯†é’¥
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "langgraph-monitoring-demo"

# å®šä¹‰å·¥ä½œæµçŠ¶æ€ç±»å‹
class ProblemSolvingState(TypedDict):
    """å¤šæ­¥éª¤é—®é¢˜è§£å†³å·¥ä½œæµçš„çŠ¶æ€"""
    problem: str  # åŸå§‹é—®é¢˜
    plan: Optional[List[str]]  # è§£å†³é—®é¢˜çš„æ­¥éª¤è®¡åˆ’
    research: Optional[Dict[str, str]]  # ç›¸å…³ç ”ç©¶/ä¿¡æ¯
    solution_draft: Optional[str]  # åˆæ­¥è§£å†³æ–¹æ¡ˆ
    solution_final: Optional[str]  # æœ€ç»ˆè§£å†³æ–¹æ¡ˆ
    evaluation: Optional[Dict[str, Any]]  # è§£å†³æ–¹æ¡ˆçš„è‡ªæˆ‘è¯„ä¼°
    current_step: str  # å½“å‰æ­¥éª¤æ ‡è¯†ç¬¦
    timestamps: Dict[str, str]  # è®°å½•æ¯ä¸ªæ­¥éª¤çš„æ—¶é—´æˆ³

# åˆ›å»ºå·¥ä½œæµèŠ‚ç‚¹
def planner(state: ProblemSolvingState) -> ProblemSolvingState:
    """åˆ¶å®šè§£å†³é—®é¢˜çš„è®¡åˆ’"""
    print("ğŸ” åˆ¶å®šè§£å†³æ–¹æ¡ˆè®¡åˆ’...")
    
    problem = state["problem"]
    
    # æ›´æ–°æ—¶é—´æˆ³
    timestamps = state.get("timestamps", {})
    timestamps["plan_start"] = datetime.now().isoformat()
    
    # ä½¿ç”¨LLMåˆ¶å®šè§£å†³é—®é¢˜çš„è®¡åˆ’
    prompt = ChatPromptTemplate.from_messages([
        SystemMessage(content="ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é—®é¢˜è§£å†³ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¸ºå¤æ‚é—®é¢˜åˆ¶å®šæ¸…æ™°çš„è§£å†³æ­¥éª¤ã€‚"),
        HumanMessage(content=f"è¯·ä¸ºä»¥ä¸‹é—®é¢˜åˆ¶å®šä¸€ä¸ª3-5æ­¥çš„è§£å†³æ–¹æ¡ˆè®¡åˆ’ã€‚æ¯ä¸€æ­¥åº”è¯¥ç®€æ˜æ‰¼è¦ã€‚\n\né—®é¢˜ï¼š{problem}")
    ])
    
    model = ChatOpenAI(temperature=0.7)
    response = model.invoke(prompt.format_messages())
    
    # å¤„ç†å“åº”ï¼Œæå–è®¡åˆ’æ­¥éª¤
    plan_text = response.content
    plan_steps = [step.strip() for step in plan_text.split("\n") if step.strip()]
    
    # è¿‡æ»¤æ‰ä¸æ˜¯çœŸæ­£æ­¥éª¤çš„è¡Œï¼ˆå¦‚æ ‡é¢˜ç­‰ï¼‰
    filtered_steps = []
    for step in plan_steps:
        # ç®€å•çš„å¯å‘å¼æ–¹æ³•ï¼šå¦‚æœè¡Œä»¥æ•°å­—æˆ–"-"æˆ–"*"å¼€å¤´ï¼Œå¯èƒ½æ˜¯ä¸€ä¸ªæ­¥éª¤
        if any(step.startswith(prefix) for prefix in ["1", "2", "3", "4", "5", "6", "7", "8", "9", "0", "-", "*", "æ­¥éª¤"]):
            filtered_steps.append(step)
    
    # å¦‚æœè¿‡æ»¤åæ²¡æœ‰æ­¥éª¤ï¼Œä½¿ç”¨åŸå§‹è¡Œ
    if not filtered_steps and plan_steps:
        filtered_steps = plan_steps
    
    # æ›´æ–°æ—¶é—´æˆ³
    timestamps["plan_end"] = datetime.now().isoformat()
    
    # è¿”å›æ›´æ–°åçš„çŠ¶æ€
    return {
        "plan": filtered_steps,
        "current_step": "research",
        "timestamps": timestamps
    }

def researcher(state: ProblemSolvingState) -> ProblemSolvingState:
    """æ”¶é›†è§£å†³é—®é¢˜æ‰€éœ€çš„ä¿¡æ¯"""
    print("ğŸ” æ”¶é›†ç›¸å…³ä¿¡æ¯...")
    
    problem = state["problem"]
    plan = state.get("plan", [])
    
    # æ›´æ–°æ—¶é—´æˆ³
    timestamps = state.get("timestamps", {})
    timestamps["research_start"] = datetime.now().isoformat()
    
    # å‡†å¤‡ç ”ç©¶æç¤º
    research_prompt = f"""ä½œä¸ºä¸€åç ”ç©¶ä¸“å®¶ï¼Œè¯·ä¸ºè§£å†³ä»¥ä¸‹é—®é¢˜æ”¶é›†å…³é”®ä¿¡æ¯ï¼š
    
    é—®é¢˜ï¼š{problem}
    
    å·²åˆ¶å®šçš„è®¡åˆ’ï¼š
    {"".join([f"- {step}\n" for step in plan])}
    
    è¯·æä¾›è§£å†³æ­¤é—®é¢˜æ‰€éœ€çš„3ä¸ªå…³é”®ä¿¡æ¯ç‚¹ã€‚æ¯ä¸ªä¿¡æ¯ç‚¹åº”åŒ…å«ï¼š
    1) æ ‡é¢˜
    2) ç®€çŸ­ä½†ä¿¡æ¯ä¸°å¯Œçš„å†…å®¹ï¼ˆ100-150å­—ï¼‰
    
    ä»¥"ä¿¡æ¯ç‚¹1ï¼š"ã€"ä¿¡æ¯ç‚¹2ï¼š"ç­‰æ ¼å¼å¼€å§‹æ¯ä¸ªéƒ¨åˆ†ã€‚
    """
    
    # ä½¿ç”¨LLMè¿›è¡Œç ”ç©¶
    model = ChatOpenAI(temperature=0.7)
    response = model.invoke([HumanMessage(content=research_prompt)])
    
    # å¤„ç†å“åº”ï¼Œæå–ç ”ç©¶ç»“æœ
    research_text = response.content
    
    # ç®€å•çš„è§£æï¼Œæå–ä¿¡æ¯ç‚¹
    info_points = {}
    current_point = None
    current_content = []
    
    for line in research_text.split('\n'):
        line = line.strip()
        if not line:
            continue
            
        # æ£€æµ‹æ–°çš„ä¿¡æ¯ç‚¹æ ‡é¢˜
        if line.startswith("ä¿¡æ¯ç‚¹") and ":" in line:
            # ä¿å­˜ä¹‹å‰çš„ä¿¡æ¯ç‚¹
            if current_point and current_content:
                info_points[current_point] = '\n'.join(current_content)
                current_content = []
                
            # å¼€å§‹æ–°çš„ä¿¡æ¯ç‚¹
            current_point = line.split(":", 1)[0].strip()
            
            # å¦‚æœæ ‡é¢˜åæœ‰å†…å®¹ï¼Œæ·»åŠ åˆ°å½“å‰å†…å®¹
            if ":" in line and len(line.split(":", 1)[1].strip()) > 0:
                current_content.append(line.split(":", 1)[1].strip())
        else:
            # ç»§ç»­æ·»åŠ åˆ°å½“å‰ä¿¡æ¯ç‚¹
            if current_point:
                current_content.append(line)
    
    # æ·»åŠ æœ€åä¸€ä¸ªä¿¡æ¯ç‚¹
    if current_point and current_content:
        info_points[current_point] = '\n'.join(current_content)
    
    # æ›´æ–°æ—¶é—´æˆ³
    timestamps["research_end"] = datetime.now().isoformat()
    
    # è¿”å›æ›´æ–°åçš„çŠ¶æ€
    return {
        "research": info_points,
        "current_step": "draft",
        "timestamps": timestamps
    }

def solution_drafter(state: ProblemSolvingState) -> ProblemSolvingState:
    """èµ·è‰åˆæ­¥è§£å†³æ–¹æ¡ˆ"""
    print("ğŸ” èµ·è‰åˆæ­¥è§£å†³æ–¹æ¡ˆ...")
    
    problem = state["problem"]
    plan = state.get("plan", [])
    research = state.get("research", {})
    
    # æ›´æ–°æ—¶é—´æˆ³
    timestamps = state.get("timestamps", {})
    timestamps["draft_start"] = datetime.now().isoformat()
    
    # å‡†å¤‡ç ”ç©¶ä¿¡æ¯æ–‡æœ¬
    research_text = ""
    for title, content in research.items():
        research_text += f"{title}:\n{content}\n\n"
    
    # å‡†å¤‡èµ·è‰æç¤º
    draft_prompt = f"""ä½œä¸ºä¸€åè§£å†³æ–¹æ¡ˆä¸“å®¶ï¼Œè¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ä¸ºé—®é¢˜è‰æ‹Ÿä¸€ä¸ªåˆæ­¥è§£å†³æ–¹æ¡ˆï¼š
    
    é—®é¢˜ï¼š{problem}
    
    è§£å†³è®¡åˆ’ï¼š
    {"".join([f"- {step}\n" for step in plan])}
    
    ç ”ç©¶ä¿¡æ¯ï¼š
    {research_text}
    
    è¯·æä¾›ä¸€ä¸ªå…¨é¢ä½†ç®€æ´çš„åˆæ­¥è§£å†³æ–¹æ¡ˆï¼Œåº”ç”¨ä¸Šè¿°ä¿¡æ¯å’Œè®¡åˆ’æ­¥éª¤ã€‚
    """
    
    # ä½¿ç”¨LLMèµ·è‰è§£å†³æ–¹æ¡ˆ
    model = ChatOpenAI(temperature=0.7)
    response = model.invoke([HumanMessage(content=draft_prompt)])
    
    # å¤„ç†å“åº”
    draft = response.content
    
    # æ›´æ–°æ—¶é—´æˆ³
    timestamps["draft_end"] = datetime.now().isoformat()
    
    # è¿”å›æ›´æ–°åçš„çŠ¶æ€
    return {
        "solution_draft": draft,
        "current_step": "refine",
        "timestamps": timestamps
    }

def solution_refiner(state: ProblemSolvingState) -> ProblemSolvingState:
    """å®Œå–„è§£å†³æ–¹æ¡ˆ"""
    print("ğŸ” å®Œå–„æœ€ç»ˆè§£å†³æ–¹æ¡ˆ...")
    
    problem = state["problem"]
    draft = state.get("solution_draft", "")
    plan = state.get("plan", [])
    
    # æ›´æ–°æ—¶é—´æˆ³
    timestamps = state.get("timestamps", {})
    timestamps["refine_start"] = datetime.now().isoformat()
    
    # å‡†å¤‡å®Œå–„æç¤º
    refine_prompt = f"""ä½œä¸ºä¸€åè§£å†³æ–¹æ¡ˆä¸“å®¶ï¼Œè¯·å®Œå–„ä»¥ä¸‹åˆæ­¥è§£å†³æ–¹æ¡ˆï¼š
    
    é—®é¢˜ï¼š{problem}
    
    è§£å†³è®¡åˆ’ï¼š
    {"".join([f"- {step}\n" for step in plan])}
    
    åˆæ­¥è§£å†³æ–¹æ¡ˆï¼š
    {draft}
    
    è¯·å®Œå–„ä¸Šè¿°è§£å†³æ–¹æ¡ˆï¼Œç¡®ä¿å®ƒï¼š
    1. ç›´æ¥è§£å†³æ ¸å¿ƒé—®é¢˜
    2. éµå¾ªè®¡åˆ’ä¸­çš„æ‰€æœ‰æ­¥éª¤
    3. é€»è¾‘æ¸…æ™°ã€è¡¨è¾¾å‡†ç¡®
    4. å®ç”¨ä¸”å¯æ‰§è¡Œ
    
    æä¾›æœ€ç»ˆçš„å®Œæ•´è§£å†³æ–¹æ¡ˆã€‚
    """
    
    # ä½¿ç”¨LLMå®Œå–„è§£å†³æ–¹æ¡ˆ
    model = ChatOpenAI(temperature=0.7)
    response = model.invoke([HumanMessage(content=refine_prompt)])
    
    # å¤„ç†å“åº”
    refined_solution = response.content
    
    # æ›´æ–°æ—¶é—´æˆ³
    timestamps["refine_end"] = datetime.now().isoformat()
    
    # è¿”å›æ›´æ–°åçš„çŠ¶æ€
    return {
        "solution_final": refined_solution,
        "current_step": "evaluate",
        "timestamps": timestamps
    }

def evaluator(state: ProblemSolvingState) -> ProblemSolvingState:
    """è¯„ä¼°æœ€ç»ˆè§£å†³æ–¹æ¡ˆ"""
    print("ğŸ” è¯„ä¼°è§£å†³æ–¹æ¡ˆ...")
    
    problem = state["problem"]
    solution = state.get("solution_final", "")
    
    # æ›´æ–°æ—¶é—´æˆ³
    timestamps = state.get("timestamps", {})
    timestamps["evaluate_start"] = datetime.now().isoformat()
    
    # å‡†å¤‡è¯„ä¼°æç¤º
    eval_prompt = f"""ä½œä¸ºä¸€åè¯„ä¼°ä¸“å®¶ï¼Œè¯·è¯„ä¼°ä»¥ä¸‹è§£å†³æ–¹æ¡ˆçš„è´¨é‡ï¼š
    
    é—®é¢˜ï¼š{problem}
    
    è§£å†³æ–¹æ¡ˆï¼š
    {solution}
    
    è¯·ä»ä»¥ä¸‹äº”ä¸ªæ–¹é¢è¯„ä¼°è¯¥è§£å†³æ–¹æ¡ˆï¼Œä¸ºæ¯ä¸ªæ–¹é¢æ‰“åˆ†(1-10)å¹¶æä¾›ç®€çŸ­ç†ç”±ï¼š
    1. æœ‰æ•ˆæ€§ï¼šè§£å†³æ–¹æ¡ˆèƒ½å¦è§£å†³æ ¸å¿ƒé—®é¢˜ï¼Ÿ
    2. å®Œæ•´æ€§ï¼šè§£å†³æ–¹æ¡ˆæ˜¯å¦å…¨é¢è¦†ç›–äº†æ‰€æœ‰å¿…è¦çš„æ–¹é¢ï¼Ÿ
    3. å¯è¡Œæ€§ï¼šè§£å†³æ–¹æ¡ˆåœ¨å®é™…ä¸­å¯ä»¥å®æ–½å—ï¼Ÿ
    4. æ¸…æ™°åº¦ï¼šè§£å†³æ–¹æ¡ˆè¡¨è¿°æ˜¯å¦æ¸…æ™°ï¼Ÿ
    5. åˆ›æ–°æ€§ï¼šè§£å†³æ–¹æ¡ˆæ˜¯å¦æä¾›äº†æ–°é¢–çš„æ€è·¯ï¼Ÿ
    
    è¯·ä»¥JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼ŒåŒ…å«å„é¡¹åˆ†æ•°å’Œç†ç”±ã€‚
    """
    
    # ä½¿ç”¨LLMè¯„ä¼°è§£å†³æ–¹æ¡ˆ
    model = ChatOpenAI(temperature=0.2)
    response = model.invoke([HumanMessage(content=eval_prompt)])
    
    # å¤„ç†å“åº”
    evaluation_text = response.content
    
    # ç®€å•è§£æè¯„ä¼°ç»“æœï¼ˆå®é™…ä¸Šåº”ä½¿ç”¨ç»“æ„åŒ–è¾“å‡ºè§£æå™¨ï¼‰
    # è¿™é‡Œä½¿ç”¨ç®€å•æ–¹æ³•æå–è¯„åˆ†
    evaluation = {
        "text": evaluation_text,
        "scores": {}
    }
    
    aspects = ["æœ‰æ•ˆæ€§", "å®Œæ•´æ€§", "å¯è¡Œæ€§", "æ¸…æ™°åº¦", "åˆ›æ–°æ€§"]
    for aspect in aspects:
        # ç®€å•å¯å‘å¼æ–¹æ³•æŸ¥æ‰¾è¯„åˆ†
        for line in evaluation_text.split("\n"):
            if aspect in line and ":" in line:
                try:
                    # å°è¯•æŸ¥æ‰¾æ•°å­—è¯„åˆ†
                    for part in line.split(":")[1].split():
                        if part.strip().isdigit():
                            evaluation["scores"][aspect] = int(part.strip())
                            break
                except:
                    # å¦‚æœè§£æå¤±è´¥ï¼Œè®¾ç½®é»˜è®¤å€¼
                    evaluation["scores"][aspect] = 0
    
    # è®¡ç®—æ€»åˆ†
    scores = evaluation["scores"].values()
    if scores:
        evaluation["total_score"] = sum(scores)
        evaluation["average_score"] = sum(scores) / len(scores)
    
    # æ›´æ–°æ—¶é—´æˆ³
    timestamps["evaluate_end"] = datetime.now().isoformat()
    
    # è¿”å›æ›´æ–°åçš„çŠ¶æ€
    return {
        "evaluation": evaluation,
        "current_step": "complete",
        "timestamps": timestamps
    }

# æ„å»ºå·¥ä½œæµå›¾
def build_problem_solving_graph():
    """æ„å»ºå¹¶è¿”å›é—®é¢˜è§£å†³å·¥ä½œæµå›¾"""
    # åˆ›å»ºçŠ¶æ€å›¾æ„å»ºå™¨
    builder = StateGraph(ProblemSolvingState)
    
    # æ·»åŠ èŠ‚ç‚¹
    builder.add_node("planner", planner)
    builder.add_node("researcher", researcher)
    builder.add_node("drafter", solution_drafter)
    builder.add_node("refiner", solution_refiner)
    builder.add_node("evaluator", evaluator)
    
    # æ·»åŠ è¾¹ï¼Œå®šä¹‰å·¥ä½œæµ
    builder.add_edge("planner", "researcher")
    builder.add_edge("researcher", "drafter")
    builder.add_edge("drafter", "refiner")
    builder.add_edge("refiner", "evaluator")
    builder.add_edge("evaluator", END)
    
    # è®¾ç½®å…¥å£ç‚¹
    builder.set_entry_point("planner")
    
    # ç¼–è¯‘å›¾
    return builder.compile()

# åˆ›å»ºå’Œç®¡ç†è¯„ä¼°æ•°æ®é›†
def create_problem_dataset():
    """åˆ›å»ºæˆ–è·å–é—®é¢˜è§£å†³è¯„ä¼°æ•°æ®é›†"""
    client = Client()
    
    try:
        # å°è¯•è¯»å–ç°æœ‰æ•°æ®é›†
        dataset = client.read_dataset("problem-solving-examples")
        print(f"ä½¿ç”¨ç°æœ‰æ•°æ®é›† '{dataset.name}'")
    except:
        # åˆ›å»ºæ–°æ•°æ®é›†
        dataset = client.create_dataset(
            "problem-solving-examples",
            description="ç”¨äºè¯„ä¼°é—®é¢˜è§£å†³å·¥ä½œæµçš„ç¤ºä¾‹é›†"
        )
        print(f"åˆ›å»ºäº†æ–°æ•°æ®é›† '{dataset.name}'")
        
        # æ·»åŠ ç¤ºä¾‹é—®é¢˜
        examples = [
            {
                "problem": "å¦‚ä½•æé«˜è¿œç¨‹å›¢é˜Ÿçš„åä½œæ•ˆç‡ï¼Ÿ",
                "reference": "æé«˜è¿œç¨‹å›¢é˜Ÿæ•ˆç‡çš„è§£å†³æ–¹æ¡ˆåº”åŒ…æ‹¬ï¼š1)å»ºç«‹æ¸…æ™°çš„æ²Ÿé€šåè®®å’Œå·¥å…·é€‰æ‹©ï¼›2)å®æ–½å¼‚æ­¥ä¸åŒæ­¥å·¥ä½œæœºåˆ¶ç›¸ç»“åˆçš„ç­–ç•¥ï¼›3)ä½¿ç”¨é¡¹ç›®ç®¡ç†å·¥å…·ç¡®ä¿ä»»åŠ¡é€æ˜åº¦ï¼›4)å®šæœŸç»„ç»‡å›¢é˜Ÿå»ºè®¾æ´»åŠ¨å¢å¼ºå‡èšåŠ›ï¼›5)æä¾›é€‚å½“åŸ¹è®­å’Œèµ„æºæ”¯æŒè¿œç¨‹å·¥ä½œã€‚"
            },
            {
                "problem": "å°å‹ä¼ä¸šå¦‚ä½•æœ‰æ•ˆé™ä½è¿è¥æˆæœ¬ï¼Ÿ",
                "reference": "å°å‹ä¼ä¸šé™ä½è¿è¥æˆæœ¬çš„æ–¹æ¡ˆåº”åŒ…æ‹¬ï¼š1)å®¡æ ¸å’Œä¼˜åŒ–ç°æœ‰æµç¨‹ä»¥æ¶ˆé™¤æµªè´¹ï¼›2)åå•†ä¾›åº”å•†åˆåŒå’Œæ¢ç´¢æ‰¹é‡é‡‡è´­ä¼˜æƒ ï¼›3)è€ƒè™‘è¿œç¨‹å·¥ä½œé€‰é¡¹å‡å°‘åŠå…¬ç©ºé—´æˆæœ¬ï¼›4)å®æ–½èƒ½æºæ•ˆç‡æªæ–½ï¼›5)åˆ©ç”¨è‡ªåŠ¨åŒ–æŠ€æœ¯å‡å°‘äººå·¥æ“ä½œï¼›6)é‡‡ç”¨ç²¾ç›Šç®¡ç†åŸåˆ™ä¸æ–­è¯„ä¼°å’Œæ”¹è¿›ã€‚"
            },
            {
                "problem": "å¦‚ä½•è®¾è®¡æœ‰æ•ˆçš„å®¢æˆ·åé¦ˆæ”¶é›†ç³»ç»Ÿï¼Ÿ",
                "reference": "æœ‰æ•ˆå®¢æˆ·åé¦ˆç³»ç»Ÿåº”ï¼š1)ä½¿ç”¨å¤šæ¸ é“æ–¹æ³•(è°ƒæŸ¥ã€ç¤¾äº¤åª’ä½“ã€ç›´æ¥å¯¹è¯ç­‰)ï¼›2)è®¾è®¡ç®€çŸ­ã€é’ˆå¯¹æ€§å¼ºçš„é—®é¢˜ï¼›3)åœ¨å®¢æˆ·æ—…ç¨‹çš„å…³é”®ç‚¹æ”¶é›†åé¦ˆï¼›4)å»ºç«‹é—­ç¯ç³»ç»Ÿç¡®ä¿åé¦ˆå¾—åˆ°å›åº”ï¼›5)æä¾›æ¿€åŠ±æªæ–½é¼“åŠ±å‚ä¸ï¼›6)ä½¿ç”¨åˆ†æå·¥å…·è¯†åˆ«è¶‹åŠ¿ï¼›7)å°†è§è§£æ•´åˆåˆ°äº§å“å’ŒæœåŠ¡æ”¹è¿›ä¸­ã€‚"
            }
        ]
        
        # æ·»åŠ åˆ°æ•°æ®é›†
        for example in examples:
            client.create_example(
                inputs={"problem": example["problem"]},
                outputs={"reference": example["reference"]},
                dataset_id=dataset.id
            )
        
        print(f"å·²æ·»åŠ  {len(examples)} ä¸ªç¤ºä¾‹åˆ°æ•°æ®é›†")
    
    return dataset.id

# ä½¿ç”¨LangSmithè¯„ä¼°å·¥ä½œæµ
def evaluate_workflow(dataset_id):
    """ä½¿ç”¨LangSmithè¯„ä¼°å·¥ä½œæµæ€§èƒ½"""
    print("\nå¼€å§‹å·¥ä½œæµè¯„ä¼°æµç¨‹...")
    
    # å®šä¹‰å·¥ä½œæµå·¥å‚å‡½æ•°ï¼ˆè¿”å›ä¸€ä¸ªèƒ½å¤„ç†è¾“å…¥çš„å‡½æ•°ï¼‰
    def workflow_factory():
        # æ„å»ºé—®é¢˜è§£å†³å›¾
        graph = build_problem_solving_graph()
        
        # åˆ›å»ºåŒ…è£…å‡½æ•°ï¼Œå¤„ç†è¾“å…¥å¹¶è¿”å›æœ€ç»ˆè§£å†³æ–¹æ¡ˆ
        def invoke_with_problem(inputs):
            problem = inputs["problem"]
            
            # åˆå§‹åŒ–çŠ¶æ€
            initial_state = {
                "problem": problem,
                "plan": None,
                "research": None,
                "solution_draft": None,
                "solution_final": None,
                "evaluation": None,
                "current_step": "plan",
                "timestamps": {"start": datetime.now().isoformat()}
            }
            
            # è¿è¡Œå›¾
            final_state = graph.invoke(initial_state)
            
            # è¿”å›éœ€è¦çš„è¾“å‡º
            return {
                "solution": final_state.get("solution_final", "æœªèƒ½ç”Ÿæˆè§£å†³æ–¹æ¡ˆ")
            }
        
        return invoke_with_problem
    
    # é…ç½®è¯„ä¼°
    eval_config = RunEvalConfig(
        # è¯„ä¼°æ ‡å‡†
        evaluators=[
            "labeled_criteria",  # ä½¿ç”¨é¢„å®šä¹‰æ ‡å‡†è¯„ä¼°
            "embedding_distance"  # æ£€æŸ¥è§£å†³æ–¹æ¡ˆä¸å‚è€ƒè§£å†³æ–¹æ¡ˆçš„ç›¸ä¼¼åº¦
        ],
        # è‡ªå®šä¹‰è¯„ä¼°æ ‡å‡†
        custom_evaluators=None,
        # è¯„ä¼°æ ‡å‡†é…ç½®
        evaluation_config={
            "labeled_criteria": {
                "criteria": {
                    "ç›´æ¥æ€§": "è§£å†³æ–¹æ¡ˆç›´æ¥é’ˆå¯¹é—®é¢˜ï¼Œä¸åç¦»ä¸»é¢˜",
                    "å¯æ“ä½œæ€§": "è§£å†³æ–¹æ¡ˆæä¾›æ˜ç¡®å¯æ‰§è¡Œçš„æ­¥éª¤",
                    "å…¨é¢æ€§": "è§£å†³æ–¹æ¡ˆæ¶µç›–é—®é¢˜çš„å„ä¸ªæ–¹é¢"
                }
            }
        }
    )
    
    # è¿è¡Œè¯„ä¼°
    results = run_on_dataset(
        dataset_name="problem-solving-examples",  # æ•°æ®é›†åç§°
        llm_or_chain_factory=workflow_factory(),  # å·¥ä½œæµ
        evaluation=eval_config,  # è¯„ä¼°é…ç½®
        verbose=True,  # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        project_name="workflow-evaluation"  # LangSmithé¡¹ç›®åç§°
    )
    
    print("è¯„ä¼°å®Œæˆï¼")
    return results

# åˆ†æå·¥ä½œæµæ€§èƒ½
def analyze_workflow_performance():
    """åˆ†æå·¥ä½œæµçš„æ€§èƒ½æŒ‡æ ‡"""
    print("\nåˆ†æå·¥ä½œæµæ€§èƒ½...")
    
    # åˆ›å»ºLangSmithå®¢æˆ·ç«¯
    client = Client()
    
    try:
        # è·å–æœ€è¿‘çš„è¿è¡Œ
        runs = list(client.list_runs(
            project_name="langgraph-monitoring-demo",
            execution_order=1,  # æŒ‰æ‰§è¡Œæ—¶é—´æ’åº
            limit=10  # è·å–æœ€è¿‘10ä¸ªè¿è¡Œ
        ))
        
        if not runs:
            print("æœªæ‰¾åˆ°è¿è¡Œè®°å½•ã€‚è¯·å…ˆè¿è¡Œå·¥ä½œæµç”Ÿæˆä¸€äº›æ•°æ®ã€‚")
            return
        
        print(f"æ‰¾åˆ° {len(runs)} ä¸ªè¿è¡Œè®°å½•")
        
        # æå–æ€§èƒ½æŒ‡æ ‡
        latencies = []
        token_counts = []
        step_latencies = {
            "planner": [],
            "researcher": [],
            "drafter": [],
            "refiner": [],
            "evaluator": []
        }
        
        for run in runs:
            # æ·»åŠ æ€»å»¶è¿Ÿ
            if run.latency_ms:
                latencies.append(run.latency_ms)
            
            # æ·»åŠ ä»¤ç‰Œè®¡æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if run.metrics and "tokens" in run.metrics:
                token_counts.append(run.metrics["tokens"])
            
            # è·å–å­è¿è¡Œä»¥åˆ†ææ¯ä¸ªæ­¥éª¤
            child_runs = list(client.list_runs(
                parent_run_id=run.id
            ))
            
            for child in child_runs:
                # åŸºäºè¿è¡Œåç§°åˆ†ç±»
                for step_name in step_latencies.keys():
                    if step_name in child.name.lower() and child.latency_ms:
                        step_latencies[step_name].append(child.latency_ms)
        
        # è®¡ç®—å’Œæ˜¾ç¤ºå¹³å‡å»¶è¿Ÿ
        if latencies:
            avg_latency = sum(latencies) / len(latencies)
            print(f"å¹³å‡æ€»å»¶è¿Ÿ: {avg_latency:.2f} æ¯«ç§’")
        
        # è®¡ç®—å’Œæ˜¾ç¤ºæ¯ä¸ªæ­¥éª¤çš„å¹³å‡å»¶è¿Ÿ
        print("\næ­¥éª¤å»¶è¿Ÿåˆ†æ:")
        for step, step_lats in step_latencies.items():
            if step_lats:
                avg_step_latency = sum(step_lats) / len(step_lats)
                print(f"  {step}: {avg_step_latency:.2f} æ¯«ç§’")
        
        # åˆ›å»ºæ­¥éª¤å»¶è¿Ÿæ¡å½¢å›¾
        if any(step_latencies.values()):
            avg_step_latencies = {}
            for step, lats in step_latencies.items():
                if lats:
                    avg_step_latencies[step] = sum(lats) / len(lats)
            
            if avg_step_latencies:
                steps = list(avg_step_latencies.keys())
                values = list(avg_step_latencies.values())
                
                plt.figure(figsize=(10, 6))
                plt.bar(steps, values)
                plt.xlabel('å¤„ç†æ­¥éª¤')
                plt.ylabel('å¹³å‡å»¶è¿Ÿ (æ¯«ç§’)')
                plt.title('å·¥ä½œæµå„æ­¥éª¤å¹³å‡å»¶è¿Ÿ')
                plt.xticks(rotation=45)
                plt.tight_layout()
                plt.savefig('workflow_latency_analysis.png')
                print("\næ­¥éª¤å»¶è¿Ÿåˆ†æå›¾å·²ä¿å­˜ä¸º 'workflow_latency_analysis.png'")
        
    except Exception as e:
        print(f"åˆ†æå·¥ä½œæµæ€§èƒ½æ—¶å‡ºé”™: {str(e)}")

# è¿è¡Œç¤ºä¾‹é—®é¢˜
def run_example_problem():
    """è¿è¡Œç¤ºä¾‹é—®é¢˜å¹¶ä½¿ç”¨LangSmithè·Ÿè¸ª"""
    print("ğŸš€ è¿è¡Œé—®é¢˜è§£å†³å·¥ä½œæµç¤ºä¾‹...\n")
    
    # æ„å»ºé—®é¢˜è§£å†³å›¾
    problem_solver = build_problem_solving_graph()
    
    # è®¾ç½®ç¤ºä¾‹é—®é¢˜
    problem = "å¦‚ä½•ä¸ºæ–°åˆ›ç«‹çš„çº¿ä¸Šæ•™è‚²å¹³å°å¢åŠ ç”¨æˆ·å‚ä¸åº¦ï¼Ÿ"
    print(f"é—®é¢˜: {problem}\n")
    
    # åˆå§‹åŒ–çŠ¶æ€
    initial_state = {
        "problem": problem,
        "plan": None,
        "research": None,
        "solution_draft": None,
        "solution_final": None,
        "evaluation": None,
        "current_step": "plan",
        "timestamps": {"start": datetime.now().isoformat()}
    }
    
    # å¦‚æœä½ æƒ³è§‚å¯Ÿæ¯ä¸ªæ­¥éª¤çš„æ‰§è¡Œï¼Œå¯ä»¥ä½¿ç”¨streamæ–¹æ³•
    print("æ‰§è¡Œå·¥ä½œæµ...")
    for event, state in problem_solver.stream(initial_state):
        if event.get("type") == "node":
            node_name = event.get("node", {}).get("name", "æœªçŸ¥èŠ‚ç‚¹")
            print(f"å®Œæˆ: {node_name}, ä¸‹ä¸€æ­¥: {state.get('current_step', 'unknown')}")
    
    # è·å–æœ€ç»ˆç»“æœ
    final_state = problem_solver.invoke(initial_state)
    
    # æ˜¾ç¤ºç»“æœ
    print("\n==== å·¥ä½œæµå®Œæˆ ====\n")
    print("æœ€ç»ˆè§£å†³æ–¹æ¡ˆ:")
    print("-----------------")
    print(final_state.get("solution_final", "æœªèƒ½ç”Ÿæˆè§£å†³æ–¹æ¡ˆ"))
    print("\nè¯„ä¼°ç»“æœ:")
    print("-----------------")
    
    evaluation = final_state.get("evaluation", {})
    scores = evaluation.get("scores", {})
    
    if scores:
        for aspect, score in scores.items():
            print(f"{aspect}: {score}/10")
        
        avg = evaluation.get("average_score")
        if avg:
            print(f"\nå¹³å‡åˆ†: {avg:.1f}/10")
    
    print("\nå·¥ä½œæµæ€§èƒ½ä¿¡æ¯å·²è®°å½•åˆ°LangSmithã€‚è¯·è®¿é—®LangSmithæ§åˆ¶å°æŸ¥çœ‹è¯¦ç»†è¿½è¸ªã€‚")
    return final_state

# ä¸»å‡½æ•°
def main():
    """ä¸»å‡½æ•°ï¼Œè¿è¡Œå®Œæ•´çš„é›†æˆç¤ºä¾‹"""
    print("===== LangGraph å’Œ LangSmith é›†æˆç¤ºä¾‹ =====\n")
    
    # æ£€æŸ¥APIå¯†é’¥
    if os.environ.get("LANGCHAIN_API_KEY") == "YOUR_LANGSMITH_API_KEY":
        print("âš ï¸ è­¦å‘Š: è¯·å°†ä»£ç ä¸­çš„'YOUR_LANGSMITH_API_KEY'æ›¿æ¢ä¸ºæ‚¨çš„å®é™…LangSmith APIå¯†é’¥")
        print("ç”³è¯·APIå¯†é’¥: https://smith.langchain.com/\n")
    
    # è¿è¡Œç¤ºä¾‹é—®é¢˜
    print("\n=== ç¬¬1éƒ¨åˆ†: è¿è¡Œå¸¦æœ‰LangSmithè·Ÿè¸ªçš„LangGraphå·¥ä½œæµ ===")
    final_state = run_example_problem()
    
    # ç­‰å¾…ç”¨æˆ·ç¡®è®¤ç»§ç»­
    input("\næŒ‰å›è½¦é”®ç»§ç»­ä¸‹ä¸€éƒ¨åˆ†...\n")
    
    # åˆ›å»ºè¯„ä¼°æ•°æ®é›†
    print("\n=== ç¬¬2éƒ¨åˆ†: åˆ›å»ºå’Œä½¿ç”¨è¯„ä¼°æ•°æ®é›† ===")
    dataset_id = create_problem_dataset()
    
    # ç­‰å¾…ç”¨æˆ·ç¡®è®¤ç»§ç»­
    input("\næŒ‰å›è½¦é”®ç»§ç»­ä¸‹ä¸€éƒ¨åˆ†...\n")
    
    # åˆ†æå·¥ä½œæµæ€§èƒ½
    print("\n=== ç¬¬3éƒ¨åˆ†: åˆ†æå·¥ä½œæµæ€§èƒ½ ===")
    analyze_workflow_performance()
    
    # ç­‰å¾…ç”¨æˆ·ç¡®è®¤ç»§ç»­
    input("\næŒ‰å›è½¦é”®ç»§ç»­ä¸‹ä¸€éƒ¨åˆ†...\n")
    
    # è¯„ä¼°å·¥ä½œæµ
    print("\n=== ç¬¬4éƒ¨åˆ†: ä½¿ç”¨LangSmithè¯„ä¼°å·¥ä½œæµ ===")
    print("æ³¨æ„: æ­¤æ­¥éª¤å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´è¿è¡Œ")
    input("æŒ‰å›è½¦é”®å¼€å§‹è¯„ä¼°æˆ–æŒ‰Ctrl+Cç»ˆæ­¢...\n")
    
    try:
        evaluate_workflow(dataset_id)
    except KeyboardInterrupt:
        print("è¯„ä¼°å·²ç»ˆæ­¢")
    
    print("\n===== ç¤ºä¾‹å®Œæˆ =====")
    print("è¯·ç™»å½•LangSmithæ§åˆ¶å°æŸ¥çœ‹è¯¦ç»†è·Ÿè¸ªå’Œè¯„ä¼°ç»“æœ:")
    print("https://smith.langchain.com/")

if __name__ == "__main__":
    main()